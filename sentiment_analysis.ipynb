{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb7c40fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tanish/OpenSource/mdanalysis/benchmarks/env/5cdaef2781286cb5eaf5e683fe4abea5/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd                                       # Data handling[1]\n",
    "import numpy as np                                        # Numerical operations[1]\n",
    "import spacy                                              # NLP and aspect extraction[2]\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer  # Overall sentiment[3]\n",
    "from textblob import TextBlob                            # Fallback sentiment[4]\n",
    "from transformers import pipeline                        # Pretrained ABSA model[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "780e25ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|██████████| 5/5 [00:05<00:00,  1.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final shape: (2033, 12)\n",
      "       reviewerID        asin  \\\n",
      "0  A2PFW17GTSAY2K  B00L8754LE   \n",
      "1  A1INKWYN6XXQ6P  B00SZ3R5HA   \n",
      "2  A22PKZZK5DSONS  B00W847AA4   \n",
      "3  A1OU2FW26L47VV  B0193XB1W0   \n",
      "4  A3D09D1C6DR1QW  B00MYYVQOY   \n",
      "\n",
      "                                          reviewText  \\\n",
      "0  Was good for 9 months then 1 corner cracked th...   \n",
      "1  I have never had a selfie stick, so I was exci...   \n",
      "2  This is one of my favorite cases outside of th...   \n",
      "3  My stock S5 battery wouldn't get me through a ...   \n",
      "4  Alright, I'm attempting to be as thurough as I...   \n",
      "\n",
      "                                               image  \n",
      "0  [https://images-na.ssl-images-amazon.com/image...  \n",
      "1  [https://images-na.ssl-images-amazon.com/image...  \n",
      "2  [https://images-na.ssl-images-amazon.com/image...  \n",
      "3  [https://images-na.ssl-images-amazon.com/image...  \n",
      "4  [https://images-na.ssl-images-amazon.com/image...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import gzip\n",
    "from tqdm import tqdm  # For progress tracking (optional)\n",
    "\n",
    "file_paths = [\n",
    "    \"Data/Reviews with images/Cell_Phones_and_Accessories_5.json.gz\",\n",
    "    \"Data/Reviews with images/Magazine_Subscriptions_5.json.gz\",\n",
    "    \"Data/Reviews with images/Appliances_5 (1).json.gz\",\n",
    "    \"Data/Reviews with images/All_Beauty_5 (1).json.gz\",\n",
    "    \"Data/Reviews with images/AMAZON_FASHION_5 (1).json.gz\",\n",
    "]\n",
    "\n",
    "def process_file(file_path, sample_size=1000):\n",
    "    \"\"\"Process a single file with memory-efficient streaming\"\"\"\n",
    "    valid_rows = []\n",
    "    try:\n",
    "        # Read file line-by-line without loading entire file\n",
    "        with gzip.open(file_path, 'rt', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    record = json.loads(line)\n",
    "                    # Check if images exist and are non-empty\n",
    "                    if isinstance(record.get('image'), list) and record['image']:\n",
    "                        valid_rows.append(record)\n",
    "                except json.JSONDecodeError:\n",
    "                    continue\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Sample if we have enough valid rows\n",
    "    if len(valid_rows) > sample_size:\n",
    "        return pd.DataFrame(valid_rows).sample(n=sample_size, random_state=42)\n",
    "    return pd.DataFrame(valid_rows)\n",
    "\n",
    "# Process files incrementally to save memory\n",
    "df_chunks = []\n",
    "for path in tqdm(file_paths, desc=\"Processing files\"):\n",
    "    df_chunk = process_file(path)\n",
    "    if not df_chunk.empty:\n",
    "        df_chunks.append(df_chunk)\n",
    "\n",
    "# Combine results if we have data\n",
    "if df_chunks:\n",
    "    df_with_images = pd.concat(df_chunks, ignore_index=True)\n",
    "    print(\"\\nFinal shape:\", df_with_images.shape)\n",
    "    print(df_with_images[['reviewerID', 'asin', 'reviewText', 'image']].head())\n",
    "else:\n",
    "    print(\"No valid data found in any files\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e9cb3a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have never had a selfie stick, so I was excited to get this once and try it out.  When it came, I followed the directions to hook it up and I extend it and started taking pictures. My teenager said I was crazy taking so many pictures. But I was having so much fun trying this out. The wire that you plug into your phone, I couldn't figure out where to put it when it was contracted back down. So it was just hanging free. That was my only concern with it. But that wasn't such a big deal. I still had so much fun playing with it. It really was fun to extend it all the way out and tilt the phone up and down to take a bunch of pictures. It was so much fun. It folded down and I was able to put it in my purse. I have a Galaxy Note with a cover on it and it still fit in the selfie stick. I thought it might not because of my case. But it did and I didn't have to take my phone out of the case. I was really happy that I didn't have to take it out of it's case. I know a lot of places banned selfie sticks, but I plan on taking this everywhere and using it when I can, without embarrassing my kids.\n"
     ]
    }
   ],
   "source": [
    "print(df_with_images['reviewText'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "465e873d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows where 'reviewText' is NOT a string:\n",
      "          reviewerID        asin reviewText  \\\n",
      "220   A1RHX83VZBU3ET  B01CE4BPNU        NaN   \n",
      "1928  A1CKPC88NHMYGR  B001IKJOLW        NaN   \n",
      "1941  A1CKPC88NHMYGR  B0058YEJ5K        NaN   \n",
      "1954  A1CKPC88NHMYGR  B0014F7B98        NaN   \n",
      "1967  A1CKPC88NHMYGR  B009MA34NY        NaN   \n",
      "1980  A1CKPC88NHMYGR  B0092UF54A        NaN   \n",
      "1993  A1CKPC88NHMYGR  B005AGO4LU        NaN   \n",
      "2008  A1CKPC88NHMYGR  B010RRWKT4        NaN   \n",
      "2021  A1CKPC88NHMYGR  B014IBJKNO        NaN   \n",
      "\n",
      "                                                  image  \n",
      "220   [https://images-na.ssl-images-amazon.com/image...  \n",
      "1928  [https://images-na.ssl-images-amazon.com/image...  \n",
      "1941  [https://images-na.ssl-images-amazon.com/image...  \n",
      "1954  [https://images-na.ssl-images-amazon.com/image...  \n",
      "1967  [https://images-na.ssl-images-amazon.com/image...  \n",
      "1980  [https://images-na.ssl-images-amazon.com/image...  \n",
      "1993  [https://images-na.ssl-images-amazon.com/image...  \n",
      "2008  [https://images-na.ssl-images-amazon.com/image...  \n",
      "2021  [https://images-na.ssl-images-amazon.com/image...  \n",
      "\n",
      "Total non-string 'reviewText' entries: 9\n",
      "220     <class 'float'>\n",
      "1928    <class 'float'>\n",
      "1941    <class 'float'>\n",
      "1954    <class 'float'>\n",
      "1967    <class 'float'>\n",
      "1980    <class 'float'>\n",
      "1993    <class 'float'>\n",
      "2008    <class 'float'>\n",
      "2021    <class 'float'>\n",
      "Name: reviewText, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Print rows where 'reviewText' is not a string\n",
    "non_string_mask = ~df_with_images['reviewText'].apply(lambda x: isinstance(x, str))\n",
    "non_string_rows = df_with_images[non_string_mask]\n",
    "\n",
    "print(\"Rows where 'reviewText' is NOT a string:\")\n",
    "print(non_string_rows[['reviewerID', 'asin', 'reviewText', 'image']])\n",
    "print(f\"\\nTotal non-string 'reviewText' entries: {len(non_string_rows)}\")\n",
    "print(non_string_rows['reviewText'].apply(type))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b947a11d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>overall</th>\n",
       "      <th>vote</th>\n",
       "      <th>verified</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>style</th>\n",
       "      <th>polarity</th>\n",
       "      <th>subjectivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[https://images-na.ssl-images-amazon.com/image...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>09 14, 2015</td>\n",
       "      <td>A2PFW17GTSAY2K</td>\n",
       "      <td>B00L8754LE</td>\n",
       "      <td>Family Man</td>\n",
       "      <td>Was good for 9 months then 1 corner cracked th...</td>\n",
       "      <td>Was good for 9 months then 1 corner cracked th...</td>\n",
       "      <td>1442188800</td>\n",
       "      <td>{'Color:': ' Red', 'Package Type:': ' Standard...</td>\n",
       "      <td>0.4680</td>\n",
       "      <td>0.527173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[https://images-na.ssl-images-amazon.com/image...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>02 22, 2016</td>\n",
       "      <td>A1INKWYN6XXQ6P</td>\n",
       "      <td>B00SZ3R5HA</td>\n",
       "      <td>KidShufflingMom</td>\n",
       "      <td>I have never had a selfie stick, so I was exci...</td>\n",
       "      <td>Fun selfie stick and fits in my purse.</td>\n",
       "      <td>1456099200</td>\n",
       "      <td>{'Color:': ' Jet Black - WIRED'}</td>\n",
       "      <td>0.9903</td>\n",
       "      <td>0.444298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[https://images-na.ssl-images-amazon.com/image...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9</td>\n",
       "      <td>False</td>\n",
       "      <td>05 17, 2015</td>\n",
       "      <td>A22PKZZK5DSONS</td>\n",
       "      <td>B00W847AA4</td>\n",
       "      <td>JC</td>\n",
       "      <td>This is one of my favorite cases outside of th...</td>\n",
       "      <td>Answers the age old question \" Is that a phone...</td>\n",
       "      <td>1431820800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.9959</td>\n",
       "      <td>0.534387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[https://images-na.ssl-images-amazon.com/image...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>10</td>\n",
       "      <td>True</td>\n",
       "      <td>08 31, 2016</td>\n",
       "      <td>A1OU2FW26L47VV</td>\n",
       "      <td>B0193XB1W0</td>\n",
       "      <td>cjr</td>\n",
       "      <td>My stock S5 battery wouldn't get me through a ...</td>\n",
       "      <td>Lots of battery life!</td>\n",
       "      <td>1472601600</td>\n",
       "      <td>{'Color:': ' Black'}</td>\n",
       "      <td>0.8499</td>\n",
       "      <td>0.379821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[https://images-na.ssl-images-amazon.com/image...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>21</td>\n",
       "      <td>True</td>\n",
       "      <td>05 15, 2015</td>\n",
       "      <td>A3D09D1C6DR1QW</td>\n",
       "      <td>B00MYYVQOY</td>\n",
       "      <td>Tabz</td>\n",
       "      <td>Alright, I'm attempting to be as thurough as I...</td>\n",
       "      <td>I seriously loved this product!</td>\n",
       "      <td>1431648000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.9975</td>\n",
       "      <td>0.518260</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               image  overall vote  verified  \\\n",
       "0  [https://images-na.ssl-images-amazon.com/image...      2.0  NaN      True   \n",
       "1  [https://images-na.ssl-images-amazon.com/image...      4.0  NaN     False   \n",
       "2  [https://images-na.ssl-images-amazon.com/image...      5.0    9     False   \n",
       "3  [https://images-na.ssl-images-amazon.com/image...      4.0   10      True   \n",
       "4  [https://images-na.ssl-images-amazon.com/image...      5.0   21      True   \n",
       "\n",
       "    reviewTime      reviewerID        asin     reviewerName  \\\n",
       "0  09 14, 2015  A2PFW17GTSAY2K  B00L8754LE       Family Man   \n",
       "1  02 22, 2016  A1INKWYN6XXQ6P  B00SZ3R5HA  KidShufflingMom   \n",
       "2  05 17, 2015  A22PKZZK5DSONS  B00W847AA4               JC   \n",
       "3  08 31, 2016  A1OU2FW26L47VV  B0193XB1W0              cjr   \n",
       "4  05 15, 2015  A3D09D1C6DR1QW  B00MYYVQOY             Tabz   \n",
       "\n",
       "                                          reviewText  \\\n",
       "0  Was good for 9 months then 1 corner cracked th...   \n",
       "1  I have never had a selfie stick, so I was exci...   \n",
       "2  This is one of my favorite cases outside of th...   \n",
       "3  My stock S5 battery wouldn't get me through a ...   \n",
       "4  Alright, I'm attempting to be as thurough as I...   \n",
       "\n",
       "                                             summary  unixReviewTime  \\\n",
       "0  Was good for 9 months then 1 corner cracked th...      1442188800   \n",
       "1             Fun selfie stick and fits in my purse.      1456099200   \n",
       "2  Answers the age old question \" Is that a phone...      1431820800   \n",
       "3                              Lots of battery life!      1472601600   \n",
       "4                    I seriously loved this product!      1431648000   \n",
       "\n",
       "                                               style  polarity  subjectivity  \n",
       "0  {'Color:': ' Red', 'Package Type:': ' Standard...    0.4680      0.527173  \n",
       "1                   {'Color:': ' Jet Black - WIRED'}    0.9903      0.444298  \n",
       "2                                                NaN    0.9959      0.534387  \n",
       "3                               {'Color:': ' Black'}    0.8499      0.379821  \n",
       "4                                                NaN    0.9975      0.518260  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vader = SentimentIntensityAnalyzer()                      # VADER analyzer[3]\n",
    "\n",
    "def overall_sentiment(text):\n",
    "    scores = vader.polarity_scores(text)\n",
    "    polarity = scores[\"compound\"]                        # Range [-1,1]\n",
    "    subjectivity = TextBlob(text).sentiment.subjectivity # Range [0,1]\n",
    "    return pd.Series({\"polarity\": polarity, \"subjectivity\": subjectivity})\n",
    "\n",
    "# Apply to all reviews\n",
    "df_with_images = df_with_images[df_with_images['reviewText'].apply(lambda x: isinstance(x, str))]\n",
    "sent_scores = df_with_images[\"reviewText\"].apply(overall_sentiment)\n",
    "df2 = pd.concat([df_with_images, sent_scores], axis=1)                # Append features[1]\n",
    "df2.head()  # Display first few rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "efeef046",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")               \n",
    "\n",
    "def extract_aspects(text):\n",
    "    doc = nlp(text)\n",
    "    return [chunk.text.lower().strip() for chunk in doc.noun_chunks if len(chunk.text.split()) <= 3]\n",
    "\n",
    "# Build global aspect list from top frequent chunks\n",
    "all_chunks = df_with_images[\"reviewText\"].apply(extract_aspects).explode()\n",
    "top_aspects = all_chunks.value_counts().head(50).index.tolist()  # Top 50 aspects[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01bc5f9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 50 meaningful aspects:\n",
      "['dryer', 'rods', 'my phone', 'works', 'lots', 'town', 'my house', 'reference', 'my vent', 'common sense', 'brush', 'one rod', 'rod', 'number', 'more rods', 'gobs', '24 foot', 'our vent', 'one cycle', 'use', '=', 'phone', 'people', 'pros', 'dust', 'photos', 'phones', 'cons', 'buttons', 'money', 'my feet', 'plenty', 'usb', 'color', 'work', 'my screen', 'order', 'my pocket', 'my purse', 'edge', 'no problem', 'dp', 'course', 'screen', 'my nexus', '100%', 'bulk', 'products', '-', 'photo']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Define stopwords and uninformative terms to exclude\n",
    "stopwords = nlp.Defaults.stop_words\n",
    "custom_exclusions = {\n",
    "    'i', 'it', 'you', 'that', 'this', 'they', 'me', 'we', 'them', 'which', \n",
    "    'who', 'something', 'anything', 'everything', 'someone', 'anyone', 'some',\n",
    "    'all', 'a', 'an', 'the', 'what', 'there', 'here', 'other', 'others'\n",
    "}\n",
    "\n",
    "def extract_aspects(text):\n",
    "    \"\"\"Extract meaningful noun chunks from text\"\"\"\n",
    "    doc = nlp(text)\n",
    "    aspects = []\n",
    "    \n",
    "    for chunk in doc.noun_chunks:\n",
    "        # Filter by length and content quality\n",
    "        tokens = [token.text.lower() for token in chunk]\n",
    "        chunk_text = chunk.text.lower().strip()\n",
    "        \n",
    "        # Skip if any of these conditions are true:\n",
    "        if (\n",
    "            len(chunk) > 3 or  # Too long\n",
    "            chunk_text in custom_exclusions or  # In exclusion list\n",
    "            all(token in stopwords for token in tokens) or  # All stopwords\n",
    "            any(token in custom_exclusions for token in tokens)  # Contains excluded terms\n",
    "        ):\n",
    "            continue\n",
    "            \n",
    "        aspects.append(chunk_text)\n",
    "        \n",
    "    return aspects\n",
    "\n",
    "# Build global aspect list\n",
    "all_chunks = df_with_images[\"reviewText\"].apply(extract_aspects).explode()\n",
    "\n",
    "# Filter and get top aspects\n",
    "aspect_counts = all_chunks.value_counts()\n",
    "meaningful_aspects = [\n",
    "    aspect for aspect in aspect_counts.index\n",
    "    if not any(excl_word in aspect for excl_word in custom_exclusions)\n",
    "]\n",
    "top_aspects = aspect_counts.loc[meaningful_aspects].head(50).index.tolist()\n",
    "\n",
    "print(\"Top 50 meaningful aspects:\")\n",
    "print(top_aspects)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "549e8bd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 50 aspects:\n",
      "dryer\n",
      "rods\n",
      "my phone\n",
      "works\n",
      "lots\n",
      "town\n",
      "my house\n",
      "reference\n",
      "my vent\n",
      "common sense\n",
      "brush\n",
      "one rod\n",
      "rod\n",
      "number\n",
      "more rods\n",
      "gobs\n",
      "24 foot\n",
      "our vent\n",
      "one cycle\n",
      "use\n",
      "=\n",
      "phone\n",
      "people\n",
      "pros\n",
      "dust\n",
      "photos\n",
      "phones\n",
      "cons\n",
      "buttons\n",
      "money\n",
      "my feet\n",
      "plenty\n",
      "usb\n",
      "color\n",
      "work\n",
      "my screen\n",
      "order\n",
      "my pocket\n",
      "my purse\n",
      "edge\n",
      "no problem\n",
      "dp\n",
      "course\n",
      "screen\n",
      "my nexus\n",
      "100%\n",
      "bulk\n",
      "products\n",
      "-\n",
      "photo\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTop 50 aspects:\")\n",
    "for aspect in top_aspects:\n",
    "    print(aspect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f8df3f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Model loaded successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing aspects: 100%|██████████| 2024/2024 [01:01<00:00, 32.76it/s] \n",
      "Processing ABSA batches: 100%|██████████| 144/144 [02:29<00:00,  1.04s/it]\n",
      "Building features: 100%|██████████| 2024/2024 [00:00<00:00, 28709.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ABSA features added successfully!\n",
      "Final shape: (2033, 112)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "# Initialize device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load ABSA model and tokenizer (with error handling)\n",
    "try:\n",
    "    model_name = \"yangheng/deberta-v3-base-absa-v1.1\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)  # Disable fast tokenizer\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name).to(device)\n",
    "    print(\"Model loaded successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")\n",
    "    # Fallback to smaller model\n",
    "    model_name = \"yangheng/deberta-v3-small-absa-v1.1\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name).to(device)\n",
    "    print(\"Using smaller model as fallback\")\n",
    "\n",
    "# Batch processing function for efficiency\n",
    "def batch_process_aspect_sentiment(text_aspect_pairs, batch_size=8):\n",
    "    \"\"\"\n",
    "    Process aspect sentiment in batches for efficiency\n",
    "    Returns: dict of {(text, aspect): (label, score)}\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    num_batches = int(np.ceil(len(text_aspect_pairs) / batch_size))\n",
    "    \n",
    "    for i in tqdm(range(num_batches), desc=\"Processing ABSA batches\"):\n",
    "        batch = text_aspect_pairs[i*batch_size : (i+1)*batch_size]\n",
    "        formatted_inputs = [f\"[CLS] {text} [SEP] {aspect} [SEP]\" for text, aspect in batch]\n",
    "        \n",
    "        try:\n",
    "            inputs = tokenizer(\n",
    "                formatted_inputs,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=512,\n",
    "                return_tensors=\"pt\"\n",
    "            ).to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "            \n",
    "            probs = torch.softmax(outputs.logits, dim=1)\n",
    "            preds = torch.argmax(probs, dim=1)\n",
    "            \n",
    "            for j, (text, aspect) in enumerate(batch):\n",
    "                label_id = preds[j].item()\n",
    "                label = model.config.id2label[label_id]\n",
    "                score = probs[j, label_id].item()\n",
    "                results[(text, aspect)] = (label, score)\n",
    "                \n",
    "        except RuntimeError as e:\n",
    "            print(f\"Batch {i} failed: {e}\")\n",
    "            # Fallback to individual processing\n",
    "            for text, aspect in batch:\n",
    "                try:\n",
    "                    inputs = tokenizer(\n",
    "                        f\"[CLS] {text} [SEP] {aspect} [SEP]\",\n",
    "                        return_tensors=\"pt\",\n",
    "                        truncation=True,\n",
    "                        max_length=512\n",
    "                    ).to(device)\n",
    "                    \n",
    "                    with torch.no_grad():\n",
    "                        outputs = model(**inputs)\n",
    "                    \n",
    "                    probs = torch.softmax(outputs.logits, dim=1)\n",
    "                    label_id = torch.argmax(probs).item()\n",
    "                    label = model.config.id2label[label_id]\n",
    "                    score = probs[0, label_id].item()\n",
    "                    results[(text, aspect)] = (label, score)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Failed on ({text[:20]}..., {aspect}): {e}\")\n",
    "                    results[(text, aspect)] = ('Neutral', 0.0)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Optimized ABSA feature extraction\n",
    "def generate_absa_features(df, top_aspects, batch_size=32):\n",
    "    \"\"\"\n",
    "    Generate ABSA features with aspect filtering and batch processing\n",
    "    \"\"\"\n",
    "    # Step 1: Collect all (text, aspect) pairs to process\n",
    "    text_aspect_pairs = []\n",
    "    review_aspect_map = defaultdict(list)\n",
    "    \n",
    "    # First pass: Extract aspects and create processing list\n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Preparing aspects\"):\n",
    "        text = row['reviewText']\n",
    "        detected = set(extract_aspects(text)) & set(top_aspects)\n",
    "        \n",
    "        for aspect in detected:\n",
    "            text_aspect_pairs.append((text, aspect))\n",
    "            review_aspect_map[idx].append(aspect)\n",
    "    \n",
    "    # Step 2: Batch process all aspect-sentiment pairs\n",
    "    sentiment_results = batch_process_aspect_sentiment(text_aspect_pairs, batch_size)\n",
    "    \n",
    "    # Step 3: Build features DataFrame\n",
    "    features = []\n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Building features\"):\n",
    "        feats = {}\n",
    "        detected_aspects = review_aspect_map.get(idx, [])\n",
    "        \n",
    "        # Add features for detected aspects\n",
    "        for aspect in detected_aspects:\n",
    "            label, score = sentiment_results.get((row['reviewText'], aspect), ('Neutral', 0.0))\n",
    "            \n",
    "            if label == 'Positive':\n",
    "                polarity = score\n",
    "            elif label == 'Negative':\n",
    "                polarity = -score\n",
    "            else:\n",
    "                polarity = 0.0\n",
    "                \n",
    "            feats[f\"{aspect}_polarity\"] = polarity\n",
    "            feats[f\"{aspect}_presence\"] = 1\n",
    "        \n",
    "        # Add zero features for non-detected aspects\n",
    "        for aspect in set(top_aspects) - set(detected_aspects):\n",
    "            feats[f\"{aspect}_polarity\"] = 0.0\n",
    "            feats[f\"{aspect}_presence\"] = 0\n",
    "            \n",
    "        features.append(feats)\n",
    "    \n",
    "    return pd.DataFrame(features)\n",
    "\n",
    "# Generate ABSA features\n",
    "absa_df = generate_absa_features(df_with_images, top_aspects)\n",
    "\n",
    "# Combine with original data\n",
    "df3 = pd.concat([df_with_images, absa_df], axis=1)\n",
    "print(\"ABSA features added successfully!\")\n",
    "print(f\"Final shape: {df3.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1ca387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my nexus_polarity     0.0\n",
      "my nexus_presence     0.0\n",
      "phones_polarity       0.0\n",
      "phones_presence       0.0\n",
      "people_polarity       0.0\n",
      "people_presence       0.0\n",
      "my house_polarity     0.0\n",
      "my house_presence     0.0\n",
      "brush_polarity        0.0\n",
      "brush_presence        0.0\n",
      "edge_polarity         0.0\n",
      "edge_presence         0.0\n",
      "rods_polarity         0.0\n",
      "rods_presence         0.0\n",
      "my screen_polarity    0.0\n",
      "my screen_presence    0.0\n",
      "our vent_polarity     0.0\n",
      "our vent_presence     0.0\n",
      "products_polarity     0.0\n",
      "products_presence     0.0\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df3.iloc[0,])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a51f3f0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-zero counts per aspect polarity column:\n",
      " my phone_presence    239\n",
      "works_polarity       228\n",
      "works_presence       228\n",
      "lots_polarity        224\n",
      "lots_presence        224\n",
      "                    ... \n",
      "edge_presence         18\n",
      "edge_polarity         18\n",
      "my nexus_polarity     17\n",
      "dp_polarity           17\n",
      "photo_polarity        16\n",
      "Length: 100, dtype: int64\n",
      "\n",
      "Total non-zero polarity columns: 100\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assume df3 is your DataFrame with ABSA features appended\n",
    "# Identify all polarity columns (ending with '_polarity')\n",
    "polarity_cols = [col for col in df3.columns if (col.endswith('_polarity') or col.endswith('_presence'))]\n",
    "\n",
    "# Compute non-zero counts for each polarity column\n",
    "non_zero_counts = (df3[polarity_cols] != 0.0).sum().sort_values(ascending=False)\n",
    "\n",
    "# Display the counts\n",
    "print(\"Non-zero counts per aspect polarity column:\\n\", non_zero_counts)\n",
    "print(\"\\nTotal non-zero polarity columns:\", len(non_zero_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fb221d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
