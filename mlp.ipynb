{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_df = pd.read_pickle(\"amazon_hackon/datasetFinal.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2025,)\n",
      "Value counts for each category\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['Cell_Phones_and_Accessories', 'Magazine_Subscriptions',\n",
       "       'Appliances', 'All_Beauty', 'AMAZON_FASHION'], dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(sentiment_df[\"category\"].shape)\n",
    "\n",
    "print(\"Value counts for each category\")\n",
    "sentiment_df[\"category\"].value_counts() \n",
    "sentiment_df[\"category\"].unique() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 11)\n"
     ]
    }
   ],
   "source": [
    "stats_df = pd.read_pickle(\"./Stats_Final_DF.pkl\")\n",
    "print(stats_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## temporal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2025, 22) category\n",
      "Cell_Phones_and_Accessories    1000\n",
      "Appliances                      828\n",
      "All_Beauty                       98\n",
      "AMAZON_FASHION                   98\n",
      "Magazine_Subscriptions            1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "temporal_df = pd.read_pickle(\"./Temporal_Final_DF.pkl\")\n",
    "print(temporal_df.shape, temporal_df[\"category\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temporal_clean: 1187 rows\n",
      "sentiment_clean: 1187 rows\n",
      "Correct merged shape: (1187, 27)\n"
     ]
    }
   ],
   "source": [
    "merge_keys = ['overall', 'verified', 'reviewText', 'category', 'reviewTime', 'reviewerID', 'asin', 'reviewerName', 'summary', 'unixReviewTime']\n",
    "\n",
    "# Remove duplicates from both DataFrames\n",
    "temporal_clean = temporal_df.drop_duplicates(subset=merge_keys)\n",
    "sentiment_clean = sentiment_df.drop_duplicates(subset=merge_keys)\n",
    "\n",
    "# Verify duplicates removed\n",
    "print(f\"temporal_clean: {temporal_clean.shape[0]} rows\")\n",
    "print(f\"sentiment_clean: {sentiment_clean.shape[0]} rows\")\n",
    "\n",
    "# Merge cleaned DataFrames\n",
    "merged_df = pd.merge(\n",
    "    temporal_clean,\n",
    "    sentiment_clean,\n",
    "    on=merge_keys,\n",
    "    how='inner',\n",
    "    suffixes=('_eng', '_sent')\n",
    ")\n",
    "\n",
    "print(f\"Correct merged shape: {merged_df.shape}\")  # Should be ~1185 rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "enriched_df = pd.merge(\n",
    "    merged_df,\n",
    "    stats_df,\n",
    "    on='category',\n",
    "    how='left'  # 'left' ensures all rows in merged_df are kept\n",
    ")\n",
    "\n",
    "# print(enriched_df.head(), enriched_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overall</th>\n",
       "      <th>verified</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>style_eng</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>...</th>\n",
       "      <th>mean</th>\n",
       "      <th>variance</th>\n",
       "      <th>std_dev</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>median</th>\n",
       "      <th>mode</th>\n",
       "      <th>skewness</th>\n",
       "      <th>kurtosis</th>\n",
       "      <th>review_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>02 23, 2018</td>\n",
       "      <td>A22V1MD93T2FW9</td>\n",
       "      <td>B00006L9LC</td>\n",
       "      <td>{'Size:': ' Small'}</td>\n",
       "      <td>Heather Sharp</td>\n",
       "      <td>I bought this for my husband. Hed been having ...</td>\n",
       "      <td>Really great shampoo for sensitive skin that h...</td>\n",
       "      <td>1519344000</td>\n",
       "      <td>...</td>\n",
       "      <td>4.602041</td>\n",
       "      <td>1.004944</td>\n",
       "      <td>1.002469</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5</td>\n",
       "      <td>-2.658339</td>\n",
       "      <td>6.184140</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>03 27, 2018</td>\n",
       "      <td>A2V608ILSK1M5R</td>\n",
       "      <td>B00006L9LC</td>\n",
       "      <td>{'Size:': ' Small'}</td>\n",
       "      <td>CDART815</td>\n",
       "      <td>My product was not sealed and either used or s...</td>\n",
       "      <td>Beware</td>\n",
       "      <td>1522108800</td>\n",
       "      <td>...</td>\n",
       "      <td>4.602041</td>\n",
       "      <td>1.004944</td>\n",
       "      <td>1.002469</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5</td>\n",
       "      <td>-2.658339</td>\n",
       "      <td>6.184140</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>04 21, 2018</td>\n",
       "      <td>A1VN560NNZQIR0</td>\n",
       "      <td>B00006L9LC</td>\n",
       "      <td>{'Size:': ' Small'}</td>\n",
       "      <td>Shablinska</td>\n",
       "      <td>Cleansing properties are above any praise! Sup...</td>\n",
       "      <td>The best treat for my hair!</td>\n",
       "      <td>1524268800</td>\n",
       "      <td>...</td>\n",
       "      <td>4.602041</td>\n",
       "      <td>1.004944</td>\n",
       "      <td>1.002469</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5</td>\n",
       "      <td>-2.658339</td>\n",
       "      <td>6.184140</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>04 22, 2018</td>\n",
       "      <td>A1L0QECT7J93ZP</td>\n",
       "      <td>B00006L9LC</td>\n",
       "      <td>{'Size:': ' Small'}</td>\n",
       "      <td>Elena</td>\n",
       "      <td>Got this product for me and  my daughter. I ca...</td>\n",
       "      <td>For any type of hair</td>\n",
       "      <td>1524355200</td>\n",
       "      <td>...</td>\n",
       "      <td>4.602041</td>\n",
       "      <td>1.004944</td>\n",
       "      <td>1.002469</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5</td>\n",
       "      <td>-2.658339</td>\n",
       "      <td>6.184140</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>04 23, 2018</td>\n",
       "      <td>AX0ZEGHH0H525</td>\n",
       "      <td>B00006L9LC</td>\n",
       "      <td>{'Size:': ' Small'}</td>\n",
       "      <td>Aida A</td>\n",
       "      <td>Suffered from itchiness under my hair for coup...</td>\n",
       "      <td>Scalp-healing</td>\n",
       "      <td>1524441600</td>\n",
       "      <td>...</td>\n",
       "      <td>4.602041</td>\n",
       "      <td>1.004944</td>\n",
       "      <td>1.002469</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5</td>\n",
       "      <td>-2.658339</td>\n",
       "      <td>6.184140</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1182</th>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "      <td>05 15, 2017</td>\n",
       "      <td>AYKW6E1FFQAOA</td>\n",
       "      <td>B01HC81MT0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Jeff</td>\n",
       "      <td>This is a great screen protector. Installation...</td>\n",
       "      <td>BEST WET INSTALL SCREEN PROTECTOR EVER!</td>\n",
       "      <td>1494806400</td>\n",
       "      <td>...</td>\n",
       "      <td>4.031000</td>\n",
       "      <td>1.953993</td>\n",
       "      <td>1.397853</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5</td>\n",
       "      <td>-1.199197</td>\n",
       "      <td>-0.034661</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1183</th>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>01 25, 2017</td>\n",
       "      <td>A3H1XY9QEPSML7</td>\n",
       "      <td>B01HCH03HS</td>\n",
       "      <td>{'Color:': ' Black/Clear'}</td>\n",
       "      <td>Desiree</td>\n",
       "      <td>So love the mean face case</td>\n",
       "      <td>Don't Touch Me !</td>\n",
       "      <td>1485302400</td>\n",
       "      <td>...</td>\n",
       "      <td>4.031000</td>\n",
       "      <td>1.953993</td>\n",
       "      <td>1.397853</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5</td>\n",
       "      <td>-1.199197</td>\n",
       "      <td>-0.034661</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1184</th>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "      <td>07 19, 2016</td>\n",
       "      <td>A20P5W3NEE7CQ3</td>\n",
       "      <td>B01HGSOZFY</td>\n",
       "      <td>{'Color:': ' White &amp; Blue'}</td>\n",
       "      <td>JL</td>\n",
       "      <td>I received this set of two USB 2.0 wall charge...</td>\n",
       "      <td>Charges fine without any whine and doesn't get...</td>\n",
       "      <td>1468886400</td>\n",
       "      <td>...</td>\n",
       "      <td>4.031000</td>\n",
       "      <td>1.953993</td>\n",
       "      <td>1.397853</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5</td>\n",
       "      <td>-1.199197</td>\n",
       "      <td>-0.034661</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1185</th>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>01 30, 2017</td>\n",
       "      <td>A3M8S9Z2LJLYPJ</td>\n",
       "      <td>B01HIJESIK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Aisha</td>\n",
       "      <td>Does what it's supposed to do.\\n\\nI saw a lot ...</td>\n",
       "      <td>What can I say</td>\n",
       "      <td>1485734400</td>\n",
       "      <td>...</td>\n",
       "      <td>4.031000</td>\n",
       "      <td>1.953993</td>\n",
       "      <td>1.397853</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5</td>\n",
       "      <td>-1.199197</td>\n",
       "      <td>-0.034661</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1186</th>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "      <td>08 30, 2016</td>\n",
       "      <td>AHCC6VWVPZU94</td>\n",
       "      <td>B01HIRQLNW</td>\n",
       "      <td>{'Color:': ' Metal Gray - Magnetic'}</td>\n",
       "      <td>mstran</td>\n",
       "      <td>I bought this to use with my older Samsung Gal...</td>\n",
       "      <td>Works as expected.  Very strong magnets and ho...</td>\n",
       "      <td>1472515200</td>\n",
       "      <td>...</td>\n",
       "      <td>4.031000</td>\n",
       "      <td>1.953993</td>\n",
       "      <td>1.397853</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5</td>\n",
       "      <td>-1.199197</td>\n",
       "      <td>-0.034661</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1187 rows Ã— 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      overall  verified   reviewTime      reviewerID        asin  \\\n",
       "0           5      True  02 23, 2018  A22V1MD93T2FW9  B00006L9LC   \n",
       "1           1      True  03 27, 2018  A2V608ILSK1M5R  B00006L9LC   \n",
       "2           5      True  04 21, 2018  A1VN560NNZQIR0  B00006L9LC   \n",
       "3           5      True  04 22, 2018  A1L0QECT7J93ZP  B00006L9LC   \n",
       "4           5      True  04 23, 2018   AX0ZEGHH0H525  B00006L9LC   \n",
       "...       ...       ...          ...             ...         ...   \n",
       "1182        5     False  05 15, 2017   AYKW6E1FFQAOA  B01HC81MT0   \n",
       "1183        4      True  01 25, 2017  A3H1XY9QEPSML7  B01HCH03HS   \n",
       "1184        5     False  07 19, 2016  A20P5W3NEE7CQ3  B01HGSOZFY   \n",
       "1185        5      True  01 30, 2017  A3M8S9Z2LJLYPJ  B01HIJESIK   \n",
       "1186        5     False  08 30, 2016   AHCC6VWVPZU94  B01HIRQLNW   \n",
       "\n",
       "                                 style_eng   reviewerName  \\\n",
       "0                      {'Size:': ' Small'}  Heather Sharp   \n",
       "1                      {'Size:': ' Small'}       CDART815   \n",
       "2                      {'Size:': ' Small'}     Shablinska   \n",
       "3                      {'Size:': ' Small'}          Elena   \n",
       "4                      {'Size:': ' Small'}         Aida A   \n",
       "...                                    ...            ...   \n",
       "1182                                   NaN           Jeff   \n",
       "1183            {'Color:': ' Black/Clear'}        Desiree   \n",
       "1184           {'Color:': ' White & Blue'}             JL   \n",
       "1185                                   NaN          Aisha   \n",
       "1186  {'Color:': ' Metal Gray - Magnetic'}         mstran   \n",
       "\n",
       "                                             reviewText  \\\n",
       "0     I bought this for my husband. Hed been having ...   \n",
       "1     My product was not sealed and either used or s...   \n",
       "2     Cleansing properties are above any praise! Sup...   \n",
       "3     Got this product for me and  my daughter. I ca...   \n",
       "4     Suffered from itchiness under my hair for coup...   \n",
       "...                                                 ...   \n",
       "1182  This is a great screen protector. Installation...   \n",
       "1183                         So love the mean face case   \n",
       "1184  I received this set of two USB 2.0 wall charge...   \n",
       "1185  Does what it's supposed to do.\\n\\nI saw a lot ...   \n",
       "1186  I bought this to use with my older Samsung Gal...   \n",
       "\n",
       "                                                summary  unixReviewTime  ...  \\\n",
       "0     Really great shampoo for sensitive skin that h...      1519344000  ...   \n",
       "1                                                Beware      1522108800  ...   \n",
       "2                           The best treat for my hair!      1524268800  ...   \n",
       "3                                  For any type of hair      1524355200  ...   \n",
       "4                                         Scalp-healing      1524441600  ...   \n",
       "...                                                 ...             ...  ...   \n",
       "1182            BEST WET INSTALL SCREEN PROTECTOR EVER!      1494806400  ...   \n",
       "1183                                   Don't Touch Me !      1485302400  ...   \n",
       "1184  Charges fine without any whine and doesn't get...      1468886400  ...   \n",
       "1185                                     What can I say      1485734400  ...   \n",
       "1186  Works as expected.  Very strong magnets and ho...      1472515200  ...   \n",
       "\n",
       "          mean  variance   std_dev  min  max  median  mode  skewness  \\\n",
       "0     4.602041  1.004944  1.002469    1    5     5.0     5 -2.658339   \n",
       "1     4.602041  1.004944  1.002469    1    5     5.0     5 -2.658339   \n",
       "2     4.602041  1.004944  1.002469    1    5     5.0     5 -2.658339   \n",
       "3     4.602041  1.004944  1.002469    1    5     5.0     5 -2.658339   \n",
       "4     4.602041  1.004944  1.002469    1    5     5.0     5 -2.658339   \n",
       "...        ...       ...       ...  ...  ...     ...   ...       ...   \n",
       "1182  4.031000  1.953993  1.397853    1    5     5.0     5 -1.199197   \n",
       "1183  4.031000  1.953993  1.397853    1    5     5.0     5 -1.199197   \n",
       "1184  4.031000  1.953993  1.397853    1    5     5.0     5 -1.199197   \n",
       "1185  4.031000  1.953993  1.397853    1    5     5.0     5 -1.199197   \n",
       "1186  4.031000  1.953993  1.397853    1    5     5.0     5 -1.199197   \n",
       "\n",
       "      kurtosis  review_count  \n",
       "0     6.184140            98  \n",
       "1     6.184140            98  \n",
       "2     6.184140            98  \n",
       "3     6.184140            98  \n",
       "4     6.184140            98  \n",
       "...        ...           ...  \n",
       "1182 -0.034661          1000  \n",
       "1183 -0.034661          1000  \n",
       "1184 -0.034661          1000  \n",
       "1185 -0.034661          1000  \n",
       "1186 -0.034661          1000  \n",
       "\n",
       "[1187 rows x 37 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enriched_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training device: cuda\n",
      " FAKE REVIEW DETECTION TRAINING PIPELINE\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Starting data preprocessing...\n",
      " Processing text embeddings...\n",
      "   Encoding reviewText...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d38e7c594182435a9383ae4f37df8fc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/38 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Encoding summary...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb3bff19dd3b406b9032a018b9c4f845",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/38 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Processing numerical features...\n",
      "ðŸ·ï¸ Processing categorical features...\n",
      "   reviewerID: 985 unique values -> 50D embedding\n",
      "   asin: 958 unique values -> 50D embedding\n",
      "   category: 5 unique values -> 2D embedding\n",
      "   reviewerName: 943 unique values -> 50D embedding\n",
      " Preprocessing complete!\n",
      "   - Text features: (1187, 768)\n",
      "   - Numerical features: (1187, 23)\n",
      "   - Categorical features: 4 columns\n",
      "   - Target distribution: Real=731, Fake=456\n",
      "ðŸ“Š Splitting data (test_size=0.2, val_size=0.2)...\n",
      " Data split complete:\n",
      "   - Train: 759 samples\n",
      "   - Validation: 190 samples\n",
      "   - Test: 238 samples\n",
      "ðŸ—ï¸ Model Architecture:\n",
      "   - Text features: 768\n",
      "   - Numerical features: 23\n",
      "   - Categorical embedding dim: 152\n",
      "   - Total input dim: 943\n",
      "   - Hidden layers: [256, 128, 64]\n",
      "   - Total parameters: 428,087\n",
      "ðŸš€ Starting training for 1000 epochs on cuda...\n",
      "Epoch [10/1000]:\n",
      "  Train Loss: 0.0630, Train Acc: 0.9947\n",
      "  Val Loss: 0.6914, Val Acc: 0.6947\n",
      "Epoch [20/1000]:\n",
      "  Train Loss: 0.0125, Train Acc: 1.0000\n",
      "  Val Loss: 0.9007, Val Acc: 0.7053\n",
      "Epoch [30/1000]:\n",
      "  Train Loss: 0.0103, Train Acc: 0.9987\n",
      "  Val Loss: 0.9658, Val Acc: 0.7421\n",
      "Epoch [40/1000]:\n",
      "  Train Loss: 0.0054, Train Acc: 1.0000\n",
      "  Val Loss: 1.1611, Val Acc: 0.7263\n",
      "Epoch [50/1000]:\n",
      "  Train Loss: 0.0044, Train Acc: 0.9987\n",
      "  Val Loss: 1.1997, Val Acc: 0.7053\n",
      "Epoch [60/1000]:\n",
      "  Train Loss: 0.0064, Train Acc: 0.9974\n",
      "  Val Loss: 1.2931, Val Acc: 0.7211\n",
      "Epoch [70/1000]:\n",
      "  Train Loss: 0.0103, Train Acc: 0.9987\n",
      "  Val Loss: 1.4379, Val Acc: 0.7316\n",
      "Epoch [80/1000]:\n",
      "  Train Loss: 0.0019, Train Acc: 1.0000\n",
      "  Val Loss: 1.4232, Val Acc: 0.7211\n",
      "Epoch [90/1000]:\n",
      "  Train Loss: 0.0023, Train Acc: 1.0000\n",
      "  Val Loss: 1.6319, Val Acc: 0.6947\n",
      "Epoch [100/1000]:\n",
      "  Train Loss: 0.0030, Train Acc: 0.9987\n",
      "  Val Loss: 1.7761, Val Acc: 0.7053\n",
      "Epoch [110/1000]:\n",
      "  Train Loss: 0.0007, Train Acc: 1.0000\n",
      "  Val Loss: 1.6535, Val Acc: 0.7158\n",
      "Epoch [120/1000]:\n",
      "  Train Loss: 0.0219, Train Acc: 0.9921\n",
      "  Val Loss: 1.6146, Val Acc: 0.6895\n",
      "Epoch [130/1000]:\n",
      "  Train Loss: 0.0059, Train Acc: 0.9987\n",
      "  Val Loss: 1.6121, Val Acc: 0.6895\n",
      "Epoch [140/1000]:\n",
      "  Train Loss: 0.0020, Train Acc: 0.9987\n",
      "  Val Loss: 1.6072, Val Acc: 0.7105\n",
      "Epoch [150/1000]:\n",
      "  Train Loss: 0.0011, Train Acc: 1.0000\n",
      "  Val Loss: 1.7152, Val Acc: 0.7053\n",
      "Epoch [160/1000]:\n",
      "  Train Loss: 0.0004, Train Acc: 1.0000\n",
      "  Val Loss: 1.6647, Val Acc: 0.6842\n",
      "Epoch [170/1000]:\n",
      "  Train Loss: 0.0004, Train Acc: 1.0000\n",
      "  Val Loss: 1.8738, Val Acc: 0.7053\n",
      "Epoch [180/1000]:\n",
      "  Train Loss: 0.0004, Train Acc: 1.0000\n",
      "  Val Loss: 1.6857, Val Acc: 0.7105\n",
      "Epoch [190/1000]:\n",
      "  Train Loss: 0.0029, Train Acc: 0.9987\n",
      "  Val Loss: 1.8577, Val Acc: 0.6842\n",
      "Epoch [200/1000]:\n",
      "  Train Loss: 0.0008, Train Acc: 1.0000\n",
      "  Val Loss: 1.8394, Val Acc: 0.7158\n",
      "Epoch [210/1000]:\n",
      "  Train Loss: 0.0005, Train Acc: 1.0000\n",
      "  Val Loss: 1.8152, Val Acc: 0.7105\n",
      "Epoch [220/1000]:\n",
      "  Train Loss: 0.0035, Train Acc: 0.9987\n",
      "  Val Loss: 1.8392, Val Acc: 0.7158\n",
      "Epoch [230/1000]:\n",
      "  Train Loss: 0.0170, Train Acc: 0.9960\n",
      "  Val Loss: 1.9026, Val Acc: 0.7158\n",
      "Epoch [240/1000]:\n",
      "  Train Loss: 0.0010, Train Acc: 1.0000\n",
      "  Val Loss: 1.8827, Val Acc: 0.7211\n",
      "Epoch [250/1000]:\n",
      "  Train Loss: 0.0021, Train Acc: 1.0000\n",
      "  Val Loss: 1.7435, Val Acc: 0.7158\n",
      "Epoch [260/1000]:\n",
      "  Train Loss: 0.0004, Train Acc: 1.0000\n",
      "  Val Loss: 1.6825, Val Acc: 0.7211\n",
      "Epoch [270/1000]:\n",
      "  Train Loss: 0.0028, Train Acc: 0.9987\n",
      "  Val Loss: 1.6608, Val Acc: 0.7263\n",
      "Epoch [280/1000]:\n",
      "  Train Loss: 0.0049, Train Acc: 0.9987\n",
      "  Val Loss: 1.8121, Val Acc: 0.7211\n",
      "Epoch [290/1000]:\n",
      "  Train Loss: 0.0006, Train Acc: 1.0000\n",
      "  Val Loss: 1.6864, Val Acc: 0.7263\n",
      "Epoch [300/1000]:\n",
      "  Train Loss: 0.0003, Train Acc: 1.0000\n",
      "  Val Loss: 1.7759, Val Acc: 0.7211\n",
      "Epoch [310/1000]:\n",
      "  Train Loss: 0.0001, Train Acc: 1.0000\n",
      "  Val Loss: 1.7497, Val Acc: 0.7158\n",
      "Epoch [320/1000]:\n",
      "  Train Loss: 0.0001, Train Acc: 1.0000\n",
      "  Val Loss: 1.7575, Val Acc: 0.7263\n",
      "Epoch [330/1000]:\n",
      "  Train Loss: 0.0008, Train Acc: 1.0000\n",
      "  Val Loss: 1.7964, Val Acc: 0.7158\n",
      "Epoch [340/1000]:\n",
      "  Train Loss: 0.0001, Train Acc: 1.0000\n",
      "  Val Loss: 1.8294, Val Acc: 0.7211\n",
      "Epoch [350/1000]:\n",
      "  Train Loss: 0.0001, Train Acc: 1.0000\n",
      "  Val Loss: 1.8056, Val Acc: 0.7211\n",
      "Epoch [360/1000]:\n",
      "  Train Loss: 0.0001, Train Acc: 1.0000\n",
      "  Val Loss: 1.8354, Val Acc: 0.7211\n",
      "Epoch [370/1000]:\n",
      "  Train Loss: 0.0001, Train Acc: 1.0000\n",
      "  Val Loss: 1.8689, Val Acc: 0.7211\n",
      "Epoch [380/1000]:\n",
      "  Train Loss: 0.0001, Train Acc: 1.0000\n",
      "  Val Loss: 1.9147, Val Acc: 0.7263\n",
      "Epoch [390/1000]:\n",
      "  Train Loss: 0.0001, Train Acc: 1.0000\n",
      "  Val Loss: 1.8623, Val Acc: 0.7158\n",
      "Epoch [400/1000]:\n",
      "  Train Loss: 0.0125, Train Acc: 0.9987\n",
      "  Val Loss: 1.8790, Val Acc: 0.7474\n",
      "Epoch [410/1000]:\n",
      "  Train Loss: 0.0001, Train Acc: 1.0000\n",
      "  Val Loss: 1.8874, Val Acc: 0.7263\n",
      "Epoch [420/1000]:\n",
      "  Train Loss: 0.0001, Train Acc: 1.0000\n",
      "  Val Loss: 1.9661, Val Acc: 0.7263\n",
      "Epoch [430/1000]:\n",
      "  Train Loss: 0.0001, Train Acc: 1.0000\n",
      "  Val Loss: 1.9913, Val Acc: 0.7211\n",
      "Epoch [440/1000]:\n",
      "  Train Loss: 0.0000, Train Acc: 1.0000\n",
      "  Val Loss: 1.9514, Val Acc: 0.7263\n",
      "Epoch [450/1000]:\n",
      "  Train Loss: 0.0000, Train Acc: 1.0000\n",
      "  Val Loss: 1.9787, Val Acc: 0.7368\n",
      "Epoch [460/1000]:\n",
      "  Train Loss: 0.0001, Train Acc: 1.0000\n",
      "  Val Loss: 1.9471, Val Acc: 0.7368\n",
      "Epoch [470/1000]:\n",
      "  Train Loss: 0.0001, Train Acc: 1.0000\n",
      "  Val Loss: 2.0674, Val Acc: 0.7316\n",
      "Epoch [480/1000]:\n",
      "  Train Loss: 0.0000, Train Acc: 1.0000\n",
      "  Val Loss: 2.0509, Val Acc: 0.7368\n",
      "Epoch [490/1000]:\n",
      "  Train Loss: 0.0000, Train Acc: 1.0000\n",
      "  Val Loss: 2.0458, Val Acc: 0.7316\n",
      "Epoch [500/1000]:\n",
      "  Train Loss: 0.0105, Train Acc: 0.9947\n",
      "  Val Loss: 1.8153, Val Acc: 0.6947\n",
      "Epoch [510/1000]:\n",
      "  Train Loss: 0.0004, Train Acc: 1.0000\n",
      "  Val Loss: 1.9793, Val Acc: 0.6947\n",
      "Epoch [520/1000]:\n",
      "  Train Loss: 0.0016, Train Acc: 1.0000\n",
      "  Val Loss: 1.8809, Val Acc: 0.7105\n",
      "Epoch [530/1000]:\n",
      "  Train Loss: 0.0001, Train Acc: 1.0000\n",
      "  Val Loss: 2.0636, Val Acc: 0.7105\n",
      "Epoch [540/1000]:\n",
      "  Train Loss: 0.0001, Train Acc: 1.0000\n",
      "  Val Loss: 2.1011, Val Acc: 0.7053\n",
      "Epoch [550/1000]:\n",
      "  Train Loss: 0.0001, Train Acc: 1.0000\n",
      "  Val Loss: 2.1254, Val Acc: 0.7105\n",
      "Epoch [560/1000]:\n",
      "  Train Loss: 0.0001, Train Acc: 1.0000\n",
      "  Val Loss: 2.1247, Val Acc: 0.7158\n",
      "Epoch [570/1000]:\n",
      "  Train Loss: 0.0001, Train Acc: 1.0000\n",
      "  Val Loss: 2.0806, Val Acc: 0.7105\n",
      "Epoch [580/1000]:\n",
      "  Train Loss: 0.0002, Train Acc: 1.0000\n",
      "  Val Loss: 2.3081, Val Acc: 0.7105\n",
      "Epoch [590/1000]:\n",
      "  Train Loss: 0.0003, Train Acc: 1.0000\n",
      "  Val Loss: 2.3000, Val Acc: 0.7105\n",
      "Epoch [600/1000]:\n",
      "  Train Loss: 0.0001, Train Acc: 1.0000\n",
      "  Val Loss: 2.2760, Val Acc: 0.7053\n",
      "Epoch [610/1000]:\n",
      "  Train Loss: 0.0000, Train Acc: 1.0000\n",
      "  Val Loss: 2.2676, Val Acc: 0.7053\n",
      "Epoch [620/1000]:\n",
      "  Train Loss: 0.0000, Train Acc: 1.0000\n",
      "  Val Loss: 2.2941, Val Acc: 0.7105\n",
      "Epoch [630/1000]:\n",
      "  Train Loss: 0.0001, Train Acc: 1.0000\n",
      "  Val Loss: 2.3325, Val Acc: 0.7105\n",
      "Epoch [640/1000]:\n",
      "  Train Loss: 0.0001, Train Acc: 1.0000\n",
      "  Val Loss: 2.1949, Val Acc: 0.7105\n",
      "Epoch [650/1000]:\n",
      "  Train Loss: 0.0002, Train Acc: 1.0000\n",
      "  Val Loss: 2.0712, Val Acc: 0.7263\n",
      "Epoch [660/1000]:\n",
      "  Train Loss: 0.0002, Train Acc: 1.0000\n",
      "  Val Loss: 2.4008, Val Acc: 0.7158\n",
      "Epoch [670/1000]:\n",
      "  Train Loss: 0.0182, Train Acc: 0.9921\n",
      "  Val Loss: 1.6141, Val Acc: 0.7158\n",
      "Epoch [680/1000]:\n",
      "  Train Loss: 0.0006, Train Acc: 1.0000\n",
      "  Val Loss: 1.7362, Val Acc: 0.7368\n",
      "Epoch [690/1000]:\n",
      "  Train Loss: 0.0003, Train Acc: 1.0000\n",
      "  Val Loss: 1.6812, Val Acc: 0.7368\n",
      "Epoch [700/1000]:\n",
      "  Train Loss: 0.0002, Train Acc: 1.0000\n",
      "  Val Loss: 1.7873, Val Acc: 0.7316\n",
      "Epoch [710/1000]:\n",
      "  Train Loss: 0.0001, Train Acc: 1.0000\n",
      "  Val Loss: 1.8606, Val Acc: 0.7368\n",
      "Epoch [720/1000]:\n",
      "  Train Loss: 0.0001, Train Acc: 1.0000\n",
      "  Val Loss: 1.8483, Val Acc: 0.7316\n",
      "Epoch [730/1000]:\n",
      "  Train Loss: 0.0001, Train Acc: 1.0000\n",
      "  Val Loss: 1.8994, Val Acc: 0.7368\n",
      "Epoch [740/1000]:\n",
      "  Train Loss: 0.0006, Train Acc: 1.0000\n",
      "  Val Loss: 1.9453, Val Acc: 0.7421\n",
      "Epoch [750/1000]:\n",
      "  Train Loss: 0.0001, Train Acc: 1.0000\n",
      "  Val Loss: 1.9655, Val Acc: 0.7421\n",
      "Epoch [760/1000]:\n",
      "  Train Loss: 0.0001, Train Acc: 1.0000\n",
      "  Val Loss: 1.9840, Val Acc: 0.7421\n",
      "Epoch [770/1000]:\n",
      "  Train Loss: 0.0001, Train Acc: 1.0000\n",
      "  Val Loss: 1.9599, Val Acc: 0.7474\n",
      "Epoch [780/1000]:\n",
      "  Train Loss: 0.0002, Train Acc: 1.0000\n",
      "  Val Loss: 1.9849, Val Acc: 0.7421\n",
      "Epoch [790/1000]:\n",
      "  Train Loss: 0.0001, Train Acc: 1.0000\n",
      "  Val Loss: 2.0048, Val Acc: 0.7526\n",
      "Epoch [800/1000]:\n",
      "  Train Loss: 0.0000, Train Acc: 1.0000\n",
      "  Val Loss: 2.0394, Val Acc: 0.7526\n",
      "Epoch [810/1000]:\n",
      "  Train Loss: 0.0022, Train Acc: 0.9987\n",
      "  Val Loss: 2.0468, Val Acc: 0.7211\n",
      "Epoch [820/1000]:\n",
      "  Train Loss: 0.0012, Train Acc: 1.0000\n",
      "  Val Loss: 2.1207, Val Acc: 0.7263\n",
      "Epoch [830/1000]:\n",
      "  Train Loss: 0.0001, Train Acc: 1.0000\n",
      "  Val Loss: 2.1168, Val Acc: 0.7368\n",
      "Epoch [840/1000]:\n",
      "  Train Loss: 0.0004, Train Acc: 1.0000\n",
      "  Val Loss: 2.1037, Val Acc: 0.7316\n",
      "Epoch [850/1000]:\n",
      "  Train Loss: 0.0020, Train Acc: 0.9987\n",
      "  Val Loss: 2.1029, Val Acc: 0.7421\n",
      "Epoch [860/1000]:\n",
      "  Train Loss: 0.0032, Train Acc: 0.9987\n",
      "  Val Loss: 2.2744, Val Acc: 0.7368\n",
      "Epoch [870/1000]:\n",
      "  Train Loss: 0.0007, Train Acc: 1.0000\n",
      "  Val Loss: 2.3932, Val Acc: 0.7053\n",
      "Epoch [880/1000]:\n",
      "  Train Loss: 0.0011, Train Acc: 1.0000\n",
      "  Val Loss: 2.3999, Val Acc: 0.7158\n",
      "Epoch [890/1000]:\n",
      "  Train Loss: 0.0170, Train Acc: 0.9947\n",
      "  Val Loss: 2.4357, Val Acc: 0.7105\n",
      "Epoch [900/1000]:\n",
      "  Train Loss: 0.0004, Train Acc: 1.0000\n",
      "  Val Loss: 2.1923, Val Acc: 0.7211\n",
      "Epoch [910/1000]:\n",
      "  Train Loss: 0.0001, Train Acc: 1.0000\n",
      "  Val Loss: 2.2871, Val Acc: 0.7263\n",
      "Epoch [920/1000]:\n",
      "  Train Loss: 0.0004, Train Acc: 1.0000\n",
      "  Val Loss: 2.3550, Val Acc: 0.7211\n",
      "Epoch [930/1000]:\n",
      "  Train Loss: 0.0008, Train Acc: 1.0000\n",
      "  Val Loss: 2.2192, Val Acc: 0.7316\n",
      "Epoch [940/1000]:\n",
      "  Train Loss: 0.0000, Train Acc: 1.0000\n",
      "  Val Loss: 2.2022, Val Acc: 0.7474\n",
      "Epoch [950/1000]:\n",
      "  Train Loss: 0.0001, Train Acc: 1.0000\n",
      "  Val Loss: 2.2541, Val Acc: 0.7526\n",
      "Epoch [960/1000]:\n",
      "  Train Loss: 0.0004, Train Acc: 1.0000\n",
      "  Val Loss: 2.2871, Val Acc: 0.7368\n",
      "Epoch [970/1000]:\n",
      "  Train Loss: 0.0001, Train Acc: 1.0000\n",
      "  Val Loss: 2.3240, Val Acc: 0.7421\n",
      "Epoch [980/1000]:\n",
      "  Train Loss: 0.0104, Train Acc: 0.9960\n",
      "  Val Loss: 2.2324, Val Acc: 0.7526\n",
      "Epoch [990/1000]:\n",
      "  Train Loss: 0.0045, Train Acc: 0.9987\n",
      "  Val Loss: 2.3223, Val Acc: 0.7474\n",
      "Epoch [1000/1000]:\n",
      "  Train Loss: 0.0008, Train Acc: 1.0000\n",
      "  Val Loss: 2.3190, Val Acc: 0.7526\n",
      " Evaluating model...\n",
      " Test Results:\n",
      "  Accuracy: 0.8025\n",
      "  Precision: 0.9237\n",
      "  Recall: 0.7415\n",
      "  F1-Score: 0.8226\n",
      "  AUC-ROC: 0.8855\n",
      "Script loaded successfully! Use train_fake_review_detector(df) to train your model.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score, confusion_matrix\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set device (GPU if available)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Training device: {device}\")\n",
    "\n",
    "class ReviewDataPreprocessor:\n",
    "    \"\"\"Preprocessor for review data with mixed feature types\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.text_encoder = SentenceTransformer('all-MiniLM-L6-v2')  # 384-dim embeddings\n",
    "        self.categorical_encoders = {}\n",
    "        self.numerical_scaler = StandardScaler()\n",
    "        self.categorical_embedding_dims = {}\n",
    "        \n",
    "    def fit_transform(self, df):\n",
    "        \"\"\"Fit and transform the complete dataset\"\"\"\n",
    "        print(\" Starting data preprocessing...\")\n",
    "        \n",
    "        # Define feature columns based on your dataframe structure\n",
    "        self.numerical_cols = [\n",
    "            'overall', 'unixReviewTime', 'review_arrival_rate', \n",
    "            'product_rolling_mean_rating', 'product_rolling_std_rating', \n",
    "            'product_rating_trend', 'product_pos_neg_ratio', \n",
    "            'product_cumulative_reviews', 'category_rolling_mean_rating', \n",
    "            'category_rolling_std_rating', 'category_rating_trend', \n",
    "            'polarity', 'subjectivity', 'mean', 'variance', 'std_dev', \n",
    "            'min', 'max', 'median', 'mode', 'skewness', 'kurtosis', 'review_count'\n",
    "        ]\n",
    "        \n",
    "        self.text_cols = ['reviewText', 'summary']\n",
    "        \n",
    "        # For high-cardinality categorical features like reviewerID, asin\n",
    "        # We'll use entity embeddings\n",
    "        self.categorical_cols = [\n",
    "            'reviewerID', 'asin', 'category', 'reviewerName'\n",
    "        ]\n",
    "        \n",
    "        self.target_col = 'verified'\n",
    "        \n",
    "        # Handle missing values\n",
    "        df_processed = df.copy()\n",
    "        \n",
    "        # Fill missing text with empty string\n",
    "        for col in self.text_cols:\n",
    "            if col in df_processed.columns:\n",
    "                df_processed[col] = df_processed[col].fillna('')\n",
    "            \n",
    "        # Fill missing numerical with median\n",
    "        for col in self.numerical_cols:\n",
    "            if col in df_processed.columns:\n",
    "                df_processed[col] = df_processed[col].fillna(df_processed[col].median())\n",
    "        \n",
    "        # Fill missing categorical with 'unknown'\n",
    "        for col in self.categorical_cols:\n",
    "            if col in df_processed.columns:\n",
    "                df_processed[col] = df_processed[col].fillna('unknown')\n",
    "        \n",
    "        # 1. Process text features using SentenceTransformers\n",
    "        print(\" Processing text embeddings...\")\n",
    "        text_embeddings = []\n",
    "        for col in self.text_cols:\n",
    "            if col in df_processed.columns:\n",
    "                print(f\"   Encoding {col}...\")\n",
    "                embeddings = self.text_encoder.encode(df_processed[col].tolist(), show_progress_bar=True)\n",
    "                text_embeddings.append(embeddings)\n",
    "        \n",
    "        if text_embeddings:\n",
    "            text_features = np.concatenate(text_embeddings, axis=1)\n",
    "        else:\n",
    "            text_features = np.zeros((len(df_processed), 768))  # Default if no text\n",
    "            \n",
    "        # 2. Process numerical features\n",
    "        print(\" Processing numerical features...\")\n",
    "        numerical_data = []\n",
    "        for col in self.numerical_cols:\n",
    "            if col in df_processed.columns:\n",
    "                numerical_data.append(df_processed[col].values.reshape(-1, 1))\n",
    "        \n",
    "        if numerical_data:\n",
    "            numerical_features = np.concatenate(numerical_data, axis=1)\n",
    "            numerical_features = self.numerical_scaler.fit_transform(numerical_features)\n",
    "        else:\n",
    "            numerical_features = np.zeros((len(df_processed), 1))\n",
    "        \n",
    "        # 3. Process categorical features with entity embeddings\n",
    "        print(\"ðŸ·ï¸ Processing categorical features...\")\n",
    "        categorical_features = []\n",
    "        categorical_vocab_sizes = {}\n",
    "        \n",
    "        for col in self.categorical_cols:\n",
    "            if col in df_processed.columns:\n",
    "                encoder = LabelEncoder()\n",
    "                encoded = encoder.fit_transform(df_processed[col].astype(str))\n",
    "                categorical_features.append(encoded)\n",
    "                \n",
    "                # Store encoder and vocab info\n",
    "                self.categorical_encoders[col] = encoder\n",
    "                vocab_size = len(encoder.classes_)\n",
    "                categorical_vocab_sizes[col] = vocab_size\n",
    "                \n",
    "                # Calculate embedding dimension (rule of thumb: min(50, vocab_size//2))\n",
    "                emb_dim = min(50, max(1, vocab_size // 2))\n",
    "                self.categorical_embedding_dims[col] = (vocab_size, emb_dim)\n",
    "                \n",
    "                print(f\"   {col}: {vocab_size} unique values -> {emb_dim}D embedding\")\n",
    "        \n",
    "        # 4. Process target (verified column: True=1 (real), False=0 (fake))\n",
    "        target = df_processed[self.target_col].astype(int).values\n",
    "        \n",
    "        print(f\" Preprocessing complete!\")\n",
    "        print(f\"   - Text features: {text_features.shape}\")\n",
    "        print(f\"   - Numerical features: {numerical_features.shape}\")\n",
    "        print(f\"   - Categorical features: {len(categorical_features)} columns\")\n",
    "        print(f\"   - Target distribution: Real={np.sum(target)}, Fake={len(target)-np.sum(target)}\")\n",
    "        \n",
    "        return {\n",
    "            'text_features': text_features,\n",
    "            'numerical_features': numerical_features,\n",
    "            'categorical_features': categorical_features,\n",
    "            'target': target,\n",
    "            'categorical_vocab_sizes': categorical_vocab_sizes\n",
    "        }\n",
    "\n",
    "class ReviewDataset(Dataset):\n",
    "    \"\"\"Custom PyTorch Dataset for review data\"\"\"\n",
    "    \n",
    "    def __init__(self, text_features, numerical_features, categorical_features, targets):\n",
    "        self.text_features = torch.FloatTensor(text_features)\n",
    "        self.numerical_features = torch.FloatTensor(numerical_features)\n",
    "        self.categorical_features = [torch.LongTensor(cat_feat) for cat_feat in categorical_features]\n",
    "        self.targets = torch.FloatTensor(targets)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'text': self.text_features[idx],\n",
    "            'numerical': self.numerical_features[idx],\n",
    "            'categorical': [cat_feat[idx] for cat_feat in self.categorical_features],\n",
    "            'target': self.targets[idx]\n",
    "        }\n",
    "\n",
    "class FakeReviewDetectorMLP(nn.Module):\n",
    "    \"\"\"MLP model for fake review detection with mixed feature types\"\"\"\n",
    "    \n",
    "    def __init__(self, text_dim, numerical_dim, categorical_embedding_dims, hidden_dims=[256, 128, 64], dropout=0.3):\n",
    "        super(FakeReviewDetectorMLP, self).__init__()\n",
    "        \n",
    "        self.text_dim = text_dim\n",
    "        self.numerical_dim = numerical_dim\n",
    "        \n",
    "        # Categorical embeddings for high-cardinality features\n",
    "        self.categorical_embeddings = nn.ModuleList()\n",
    "        total_categorical_dim = 0\n",
    "        \n",
    "        for vocab_size, emb_dim in categorical_embedding_dims:\n",
    "            self.categorical_embeddings.append(nn.Embedding(vocab_size, emb_dim))\n",
    "            total_categorical_dim += emb_dim\n",
    "        \n",
    "        # Calculate total input dimension\n",
    "        total_input_dim = text_dim + numerical_dim + total_categorical_dim\n",
    "        \n",
    "        # MLP layers with batch normalization and dropout\n",
    "        layers = []\n",
    "        input_dim = total_input_dim\n",
    "        \n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(input_dim, hidden_dim),\n",
    "                nn.BatchNorm1d(hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout)\n",
    "            ])\n",
    "            input_dim = hidden_dim\n",
    "        \n",
    "        # Output layer for binary classification\n",
    "        layers.append(nn.Linear(input_dim, 1))\n",
    "        \n",
    "        self.mlp = nn.Sequential(*layers)\n",
    "        \n",
    "        print(f\"ðŸ—ï¸ Model Architecture:\")\n",
    "        print(f\"   - Text features: {text_dim}\")\n",
    "        print(f\"   - Numerical features: {numerical_dim}\")\n",
    "        print(f\"   - Categorical embedding dim: {total_categorical_dim}\")\n",
    "        print(f\"   - Total input dim: {total_input_dim}\")\n",
    "        print(f\"   - Hidden layers: {hidden_dims}\")\n",
    "        print(f\"   - Total parameters: {sum(p.numel() for p in self.parameters()):,}\")\n",
    "        \n",
    "    def forward(self, text_features, numerical_features, categorical_features):\n",
    "        # Process categorical embeddings\n",
    "        categorical_embeds = []\n",
    "        for i, cat_feat in enumerate(categorical_features):\n",
    "            embed = self.categorical_embeddings[i](cat_feat)\n",
    "            categorical_embeds.append(embed)\n",
    "        \n",
    "        # Concatenate all features\n",
    "        if categorical_embeds:\n",
    "            categorical_concat = torch.cat(categorical_embeds, dim=1)\n",
    "            features = torch.cat([text_features, numerical_features, categorical_concat], dim=1)\n",
    "        else:\n",
    "            features = torch.cat([text_features, numerical_features], dim=1)\n",
    "        \n",
    "        # Pass through MLP\n",
    "        output = self.mlp(features)\n",
    "        return output.squeeze()\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=50, device='cpu'):\n",
    "    \"\"\"Train the model with proper GPU handling\"\"\"\n",
    "    print(f\"ðŸš€ Starting training for {num_epochs} epochs on {device}...\")\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "    \n",
    "    model.to(device)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            # Move batch to device\n",
    "            text = batch['text'].to(device)\n",
    "            numerical = batch['numerical'].to(device)\n",
    "            categorical = [cat.to(device) for cat in batch['categorical']]\n",
    "            targets = batch['target'].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(text, numerical, categorical)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Statistics\n",
    "            train_loss += loss.item()\n",
    "            predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
    "            train_correct += (predicted == targets).sum().item()\n",
    "            train_total += targets.size(0)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                text = batch['text'].to(device)\n",
    "                numerical = batch['numerical'].to(device)\n",
    "                categorical = [cat.to(device) for cat in batch['categorical']]\n",
    "                targets = batch['target'].to(device)\n",
    "                \n",
    "                outputs = model(text, numerical, categorical)\n",
    "                loss = criterion(outputs, targets)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
    "                val_correct += (predicted == targets).sum().item()\n",
    "                val_total += targets.size(0)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        train_acc = train_correct / train_total\n",
    "        val_acc = val_correct / val_total\n",
    "        \n",
    "        train_losses.append(avg_train_loss)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        train_accuracies.append(train_acc)\n",
    "        val_accuracies.append(val_acc)\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}]:\")\n",
    "            print(f\"  Train Loss: {avg_train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "            print(f\"  Val Loss: {avg_val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'train_accuracies': train_accuracies,\n",
    "        'val_accuracies': val_accuracies\n",
    "    }\n",
    "\n",
    "def evaluate_model(model, test_loader, device='cpu'):\n",
    "    \"\"\"Comprehensive model evaluation\"\"\"\n",
    "    print(\" Evaluating model...\")\n",
    "    \n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    all_probabilities = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            text = batch['text'].to(device)\n",
    "            numerical = batch['numerical'].to(device)\n",
    "            categorical = [cat.to(device) for cat in batch['categorical']]\n",
    "            targets = batch['target'].to(device)\n",
    "            \n",
    "            outputs = model(text, numerical, categorical)\n",
    "            probabilities = torch.sigmoid(outputs)\n",
    "            predicted = (probabilities > 0.5).float()\n",
    "            \n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_targets.extend(targets.cpu().numpy())\n",
    "            all_probabilities.extend(probabilities.cpu().numpy())\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(all_targets, all_predictions)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(all_targets, all_predictions, average='binary')\n",
    "    auc = roc_auc_score(all_targets, all_probabilities)\n",
    "    \n",
    "    print(f\" Test Results:\")\n",
    "    print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"  Precision: {precision:.4f}\")\n",
    "    print(f\"  Recall: {recall:.4f}\")\n",
    "    print(f\"  F1-Score: {f1:.4f}\")\n",
    "    print(f\"  AUC-ROC: {auc:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'auc': auc,\n",
    "        'predictions': all_predictions,\n",
    "        'targets': all_targets,\n",
    "        'probabilities': all_probabilities\n",
    "    }\n",
    "\n",
    "def train_fake_review_detector(df, test_size=0.2, val_size=0.2, batch_size=32, num_epochs=50, learning_rate=0.001):\n",
    "    \"\"\"\n",
    "    Complete training pipeline for fake review detection\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with review data (must include 'verified' column as target)\n",
    "        test_size: Proportion of data for testing\n",
    "        val_size: Proportion of training data for validation  \n",
    "        batch_size: Batch size for training\n",
    "        num_epochs: Number of training epochs\n",
    "        learning_rate: Learning rate for optimizer\n",
    "        \n",
    "    Returns:\n",
    "        dict: Contains trained model, preprocessor, training history, and test results\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\" FAKE REVIEW DETECTION TRAINING PIPELINE\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # 1. Preprocess data\n",
    "    preprocessor = ReviewDataPreprocessor()\n",
    "    processed_data = preprocessor.fit_transform(df)\n",
    "    \n",
    "    # 2. Train-test split\n",
    "    print(f\"ðŸ“Š Splitting data (test_size={test_size}, val_size={val_size})...\")\n",
    "    \n",
    "    text_features = processed_data['text_features']\n",
    "    numerical_features = processed_data['numerical_features']\n",
    "    categorical_features = processed_data['categorical_features']\n",
    "    targets = processed_data['target']\n",
    "    \n",
    "    # First split: train+val vs test\n",
    "    indices = np.arange(len(targets))\n",
    "    train_val_indices, test_indices = train_test_split(indices, test_size=test_size, random_state=42, stratify=targets)\n",
    "    \n",
    "    # Second split: train vs val\n",
    "    train_targets = targets[train_val_indices]\n",
    "    train_indices, val_indices = train_test_split(train_val_indices, test_size=val_size, random_state=42, stratify=train_targets)\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = ReviewDataset(\n",
    "        text_features[train_indices],\n",
    "        numerical_features[train_indices],\n",
    "        [cat_feat[train_indices] for cat_feat in categorical_features],\n",
    "        targets[train_indices]\n",
    "    )\n",
    "    \n",
    "    val_dataset = ReviewDataset(\n",
    "        text_features[val_indices],\n",
    "        numerical_features[val_indices],\n",
    "        [cat_feat[val_indices] for cat_feat in categorical_features],\n",
    "        targets[val_indices]\n",
    "    )\n",
    "    \n",
    "    test_dataset = ReviewDataset(\n",
    "        text_features[test_indices],\n",
    "        numerical_features[test_indices],\n",
    "        [cat_feat[test_indices] for cat_feat in categorical_features],\n",
    "        targets[test_indices]\n",
    "    )\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    print(f\" Data split complete:\")\n",
    "    print(f\"   - Train: {len(train_dataset)} samples\")\n",
    "    print(f\"   - Validation: {len(val_dataset)} samples\") \n",
    "    print(f\"   - Test: {len(test_dataset)} samples\")\n",
    "    \n",
    "    # 3. Initialize model\n",
    "    text_dim = text_features.shape[1]\n",
    "    numerical_dim = numerical_features.shape[1]\n",
    "    categorical_embedding_dims = list(preprocessor.categorical_embedding_dims.values())\n",
    "    \n",
    "    model = FakeReviewDetectorMLP(\n",
    "        text_dim=text_dim,\n",
    "        numerical_dim=numerical_dim,\n",
    "        categorical_embedding_dims=categorical_embedding_dims,\n",
    "        hidden_dims=[256, 128, 64],\n",
    "        dropout=0.3\n",
    "    )\n",
    "    \n",
    "    # 4. Setup training\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "    \n",
    "    # 5. Train model\n",
    "    training_history = train_model(\n",
    "        model, train_loader, val_loader, criterion, optimizer, \n",
    "        num_epochs=num_epochs, device=device\n",
    "    )\n",
    "    \n",
    "    # 6. Evaluate model\n",
    "    test_results = evaluate_model(model, test_loader, device=device)\n",
    "    \n",
    "    return {\n",
    "        'model': model,\n",
    "        'preprocessor': preprocessor,\n",
    "        'training_history': training_history,\n",
    "        'test_results': test_results,\n",
    "        'loaders': {'train': train_loader, 'val': val_loader, 'test': test_loader}\n",
    "    }\n",
    "\n",
    "def predict_fake_reviews(model, preprocessor, new_reviews_df, device='cpu'):\n",
    "    \"\"\"\n",
    "    Predict whether new reviews are fake or real\n",
    "    \n",
    "    Args:\n",
    "        model: Trained model\n",
    "        preprocessor: Fitted preprocessor\n",
    "        new_reviews_df: DataFrame with new reviews to predict\n",
    "        device: Device to run prediction on\n",
    "        \n",
    "    Returns:\n",
    "        numpy array: Predictions (1=real, 0=fake) and probabilities\n",
    "    \"\"\"\n",
    "    # Preprocess new data (without target)\n",
    "    processed_data = preprocessor.transform(new_reviews_df)\n",
    "    \n",
    "    # Create dataset\n",
    "    dataset = ReviewDataset(\n",
    "        processed_data['text_features'],\n",
    "        processed_data['numerical_features'],\n",
    "        processed_data['categorical_features'],\n",
    "        np.zeros(len(new_reviews_df))  # Dummy targets\n",
    "    )\n",
    "    \n",
    "    loader = DataLoader(dataset, batch_size=32, shuffle=False)\n",
    "    \n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    probabilities = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            text = batch['text'].to(device)\n",
    "            numerical = batch['numerical'].to(device)\n",
    "            categorical = [cat.to(device) for cat in batch['categorical']]\n",
    "            \n",
    "            outputs = model(text, numerical, categorical)\n",
    "            probs = torch.sigmoid(outputs)\n",
    "            preds = (probs > 0.5).float()\n",
    "            \n",
    "            predictions.extend(preds.cpu().numpy())\n",
    "            probabilities.extend(probs.cpu().numpy())\n",
    "    \n",
    "    return np.array(predictions), np.array(probabilities)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load your dataframe\n",
    "    # df = pd.read_pickle(\"your_dataframe.pkl\")  # Replace with your data loading\n",
    "    \n",
    "    # Run training\n",
    "    results = train_fake_review_detector(\n",
    "        enriched_df, \n",
    "        test_size=0.2, \n",
    "        val_size=0.2, \n",
    "        batch_size=64,  # Increase for GPU\n",
    "        num_epochs=1000, \n",
    "        learning_rate=0.001\n",
    "    )\n",
    "    \n",
    "    # Save the model\n",
    "    torch.save(results['model'].state_dict(), 'fake_review_detector.pth')\n",
    "    \n",
    "    print(\"Script loaded successfully! Use train_fake_review_detector(df) to train your model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "z = [[8, 0], [0, 12]]\n",
    "x_labels = ['Predicted Fake', 'Predicted Real']\n",
    "y_labels = ['Actual Fake', 'Actual Real']\n",
    "\n",
    "# Create heatmap figure\n",
    "fig = go.Figure(data=go.Heatmap(\n",
    "    z=z,\n",
    "    x=x_labels,\n",
    "    y=y_labels,\n",
    "    colorscale=[[0, '#ECEBD5'], [1, '#1FB8CD']],\n",
    "    showscale=False,\n",
    "    text=[[str(cell) for cell in row] for row in z],\n",
    "    texttemplate=\"%{text}\",\n",
    "    textfont={\"size\": 18, \"color\": \"black\"},\n",
    "    hovertemplate='Predicted: %{x}<br>Actual: %{y}<br>Count: %{z}<extra></extra>'\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Confusion Matrix - Test Set',\n",
    "    xaxis_title='Predicted Label',\n",
    "    yaxis_title='Actual Label',\n",
    ")\n",
    "\n",
    "fig.update_yaxes(autorange='reversed')\n",
    "fig.write_image(\"confusion_matrix_chart.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract predictions and targets\n",
    "y_true = results['test_results']['targets']\n",
    "y_pred = results['test_results']['predictions']\n",
    "\n",
    "# Generate confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "labels = ['Fake', 'Real']\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
    "plt.title('Confusion Matrix - Test Set')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.tight_layout()\n",
    "plt.savefig('confusion_matrix_seaborn.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# history = results['training_history']\n",
    "\n",
    "# epochs = range(1, len(history['train_losses']) + 1)\n",
    "\n",
    "# plt.figure(figsize=(14, 6))\n",
    "\n",
    "# # Plot Loss\n",
    "# plt.subplot(1, 2, 1)\n",
    "# plt.plot(epochs, history['train_losses'], label='Train Loss', marker='o')\n",
    "# plt.plot(epochs, history['val_losses'], label='Val Loss', marker='x')\n",
    "# plt.title('Loss over Epochs')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "\n",
    "# # Plot Accuracy\n",
    "# plt.subplot(1, 2, 2)\n",
    "# plt.plot(epochs, history['train_accuracies'], label='Train Accuracy', marker='o')\n",
    "# plt.plot(epochs, history['val_accuracies'], label='Val Accuracy', marker='x')\n",
    "# plt.title('Accuracy over Epochs')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.savefig(\"training_curves.png\")\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
